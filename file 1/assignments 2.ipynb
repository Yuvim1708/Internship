{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5a6405-52cf-4989-8073-e5bf706ee5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Location, Company Name, Experience Required]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.naukri.com/data-scientist-jobs-in-delhi-ncr-salary-3-to-6-lakhs\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "jobs = soup.find_all('article', class_='jobTuple bgWhite br4 mb-8')\n",
    "\n",
    "\n",
    "job_data = []\n",
    "for job in jobs[:10]:\n",
    "    title = job.find('a', class_='title').text.strip()\n",
    "    location = job.find('li', class_='location').text.strip()\n",
    "    company = job.find('a', class_='subTitle').text.strip()\n",
    "    experience = job.find('li', class_='experience').text.strip()\n",
    "    job_data.append([title, location, company, experience])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(job_data, columns=['Job Title', 'Location', 'Company Name', 'Experience Required'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5505977-3735-4e36-8829-4c71b2e17ebe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2729ed3a-133e-46fc-a58a-95b6ab281d4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'send_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Step 3: Search for \"Data Scientist\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m search_box \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msugInp\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 15\u001b[0m search_box\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Scientist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 4: Click the search button (simulated by constructing the search URL)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m search_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.naukri.com/jobs/search/jobs?keyword=Data%20Scientist&location=Delhi%2FNCR&salary=3-6\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'send_keys'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the web page\n",
    "url = \"https://www.naukri.com/\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Step 2: Parse the page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 3: Search for \"Data Scientist\"\n",
    "search_box = soup.find('input', {'class': 'sugInp'})\n",
    "search_box.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Step 4: Click the search button (simulated by constructing the search URL)\n",
    "search_url = f\"https://www.naukri.com/jobs/search/jobs?keyword=Data%20Scientist&location=Delhi%2FNCR&salary=3-6\"\n",
    "\n",
    "# Step 5: Fetch the search results page\n",
    "response = requests.get(search_url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 6: Scrape the data for the first 10 job results\n",
    "job_listings = soup.find_all('div', class_='jobTuple', limit=10)\n",
    "\n",
    "jobs = []\n",
    "for job in job_listings:\n",
    "    title = job.find('a', class_='title').text.strip()\n",
    "    location = job.find('li', class_='location').text.strip()\n",
    "    company = job.find('a', class_='subTitle').text.strip()\n",
    "    experience = job.find('li', class_='experience').text.strip()\n",
    "    \n",
    "    jobs.append({\n",
    "        'Job Title': title,\n",
    "        'Location': location,\n",
    "        'Company': company,\n",
    "        'Experience Required': experience\n",
    "    })\n",
    "\n",
    "# Step 7: Create a DataFrame\n",
    "df_jobs = pd.DataFrame(jobs)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401c262-9c4a-4c18-a17e-2b85e225aa68",
   "metadata": {},
   "source": [
    "\n",
    "2. **WebDriver**: You will need a WebDriver for your browser (e.g., ChromeDriver for Google Chrome). Make sure to download the appropriate version of the WebDriver and place it in your system's PATH.\n",
    "\n",
    "### Scraping Steps with Selenium\n",
    "1. **Open the Naukri website**.\n",
    "2. **Enter \"Data Scientist\" in the search field**.\n",
    "3. **Click the search button**.\n",
    "4. **Apply the location and salary filters**.\n",
    "5. **Scrape the data for the first 10 job results**.\n",
    "6. **Create a DataFrame of the scraped data**.\n",
    "\n",
    "Here is the code that implements the above steps using Selenium:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18f43774-feb8-49a1-a5ce-1c98364080a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keys\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Step 1: Set up the WebDriver (Make sure to specify the correct path to your WebDriver)\n",
    "driver = webdriver.Chrome(executable_path='path/to/chromedriver')\n",
    "\n",
    "# Step 2: Open the Naukri website\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# Step 3: Enter \"Data Scientist\" in the search field\n",
    "search_box = driver.find_element(By.CLASS_NAME, 'sugInp')\n",
    "search_box.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Step 4: Click the search button\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 5: Apply the location and salary filters\n",
    "# Click on the location filter\n",
    "location_filter = driver.find_element(By.XPATH, \"//label[contains(text(), 'Delhi/NCR')]\")\n",
    "location_filter.click()\n",
    "\n",
    "# Click on the salary filter\n",
    "salary_filter = driver.find_element(By.XPATH, \"//label[contains(text(), '3-6 Lakhs')]\")\n",
    "salary_filter.click()\n",
    "\n",
    "# Wait for the page to load the filtered results\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 6: Scrape the data for the first 10 job results\n",
    "job_listings = driver.find_elements(By.CLASS_NAME, 'jobTuple')[:10]\n",
    "\n",
    "jobs = []\n",
    "for job in job_listings:\n",
    "    title = job.find_element(By.CLASS_NAME, 'title').text.strip()\n",
    "    location = job.find_element(By.CLASS_NAME, 'location').text.strip()\n",
    "    company = job.find_element(By.CLASS_NAME, 'subTitle').text.strip()\n",
    "    experience = job.find_element(By.CLASS_NAME, 'experience').text.strip()\n",
    "    \n",
    "    jobs.append({\n",
    "        'Job Title': title,\n",
    "        'Location': location,\n",
    "        'Company': company,\n",
    "        'Experience Required': experience\n",
    "    })\n",
    "\n",
    "# Step 7: Create a DataFrame\n",
    "df_jobs = pd.DataFrame(jobs)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_jobs)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df9d0c-f634-40ee-8ec3-1a29a0fd4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.shine.com/job-search/data-scientist-jobs-in-bangalore\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "jobs = soup.find_all('li', class_='result-display__profile')\n",
    "\n",
    "\n",
    "job_data = []\n",
    "for job in jobs[:10]:\n",
    "    title = job.find('h3', class_='result-display__profile__job__title').text.strip()\n",
    "    location = job.find('div', class_='result-display__profile__location').text.strip()\n",
    "    company = job.find('div', class_='result-display__profile__company-name').text.strip()\n",
    "    experience = job.find('div', class_='result-display__profile__experience').text.strip()\n",
    "    job_data.append([title, location, company, experience])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(job_data, columns=['Job Title', 'Location', 'Company Name', 'Experience Required'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f105a-a202-4ff8-af30-26c8cfcb04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "reviews = soup.find_all('div', class_='col _2wzgFH K0kLPL')\n",
    "\n",
    "\n",
    "review_data = []\n",
    "for review in reviews[:100]:\n",
    "    rating = review.find('div', class_='_3LWZlK _1BLPMq').text\n",
    "    summary = review.find('p', class_='_2-N8zT').text\n",
    "    full_review = review.find('div', class_='t-ZTKy').text.strip()\n",
    "    review_data.append([rating, summary, full_review])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(review_data, columns=['Rating', 'Review Summary', 'Full Review'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefeacfb-70cc-4fbf-b2e4-343dac24fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.flipkart.com/search?q=sneakers\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "products = soup.find_all('div', class_='_2B099V')\n",
    "\n",
    "\n",
    "sneaker_data = []\n",
    "for product in products[:100]:\n",
    "    brand = product.find('div', class_='_2WkVRV').text.strip()\n",
    "    description = product.find('a', class_='IRpwTa').text.strip()\n",
    "    price = product.find('div', class_='_30jeq3').text.strip()\n",
    "    sneaker_data.append([brand, description, price])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(sneaker_data, columns=['Brand', 'Product Description', 'Price'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce079d9-232e-43bf-9e06-41a4eef65a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.amazon.in/s?k=laptops+intel+core+i7\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "products = soup.find_all('div', class_='s-asin')\n",
    "\n",
    "\n",
    "laptop_data = []\n",
    "for product in products[:10]:\n",
    "    title = product.find('span', class_='a-size-medium a-color-base a-text-normal').text.strip()\n",
    "    price = product.find('span', class_='a-price-whole').text.strip()\n",
    "    rating = product.find('span', class_='a-icon-alt').text.strip()\n",
    "    laptop_data.append([title, rating, price])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(laptop_data, columns=['Title', 'Rating', 'Price'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07506d7-ca1d-4875-a139-4998713707c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.azquotes.com/top_quotes.html\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "quotes = soup.find_all('div', class_='quote')\n",
    "\n",
    "\n",
    "quote_data = []\n",
    "for quote in quotes[:1000]:\n",
    "    text = quote.find('a', class_='title').text.strip()\n",
    "    author = quote.find('div', class_='author').text.strip()\n",
    "    tags = quote.find('div', class_='tags').text.strip()\n",
    "    quote_data.append([text, author, tags])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(quote_data, columns=['Quote', 'Author', 'Tags'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220707d1-0e28-489b-a05a-3899319adc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('table')\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "\n",
    "pm_data = []\n",
    "for row in rows[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    name = cols[0].text.strip()\n",
    "    born_dead = cols[1].text.strip()\n",
    "    term = cols[2].text.strip()\n",
    "    remarks = cols[3].text.strip()\n",
    "    pm_data.append([name, born_dead, term, remarks])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(pm_data, columns=['Name', 'Born-Dead', 'Term of Office', 'Remarks'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195127d-028d-459d-8b42-5726d9813564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.motor1.com/features/262513/most-expensive-cars-world/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "cars = soup.find_all('div', class_='article-body')[0].find_all('h2')\n",
    "\n",
    "\n",
    "car_data = []\n",
    "for car in cars[:50]:\n",
    "    name = car.text.strip()\n",
    "    price = car.find_next_sibling('p').text.strip()\n",
    "    car_data.append([name, price])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(car_data, columns=['Car Name', 'Price'])\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
